# Expectation
The concept of the expected value of a function of a random variable gives us a unified framework in which to look at mean variance of a random variable, together with mean and variance of sums of random variables. The expectation of a function of a random variable has a very similar definition for discrete and for continuous distributions. In the discrete case, we have a discrete random variable.

Our sample space is a subset of the reals. Our events are specified according to the rules for events. And P is a probability function mapping the events to the reals between 0 and 1. Let's say it's induced by a density function, f, and g is a function mapping the sample space, again, to the reals.

Then the expected value of g of x equals the sum over all outcomes of g of x times f of x. And we denote it with this notation cap E square brackets g of x, or the expected value of g of x. Likewise, if you have a continuous random variable and f is the density function for that and g maps the sample space to the reals, then the expected value of g of x is the integral of g of x times f of x dx. And it's again denoted, expected value of g of x with this cap E in square brackets notation.

In fact, these definitions coincide with the mean of the random variable induced by g of x, y equals g of x. And it's relatively easy to see this in the discrete case. And it's true generally. Let's have a look at this in the discrete case.

So let's verify the expected value of g of x with respect to the probability distribution of x equals the expected value of y with respect to the probability distribution of y in a particular example, and then we'll generalize this. So let our probability space just be the model for rolling a fair die and recording the number that's rolled. Let g of x be this fairly arbitrary random variable defined by g of 1 equals 0, g of 2 and 3 equals 1, g of 4 and 5 equals 2, and g of 6 equals 3.

Then according to our definition, the expected value of g of x equals the sum overall outcomes-- so values 1 through 6-- of g of that outcome times the probability of that outcome. Our outcomes all have probability 1/6. So we have that probability written here.

If we roll a 1, that's 0 times 1/6. If we roll a 2 or a 3, then we have 1 times 1/6 for the 2 and 1 times 1/6 for the 3. And let's group it that way.

If we roll a 4, we record a 2 times 1/6. And if we roll a 5, we also have a 2 times 1/6. So let's group them together that way. And finally, if we roll a 6, g of x is 3. And the probability is 1/6. So adding that all up, that's 2/3 here plus 1/3 plus 1/2, bringing us up to 3/2.

We can view this in the probability space induced by g by saying that the sample space now is just 0, 1, 2, 3-- the possible outcomes from g of x-- the outcomes in y. The probability of 0 is 1/6. The probability of 1, we could get that from two different rolls, so it's 1/6 plus 1/6, or 1/3.

The probability of a 2 is 1/6 plus 1/6, because that also was generated by two different rolls. And the probability of 3 is 1/6. So now if we look at the expected value in this probability space of the identity function y-- so we're just taking the identity function-- the value itself times its probability.

Then we're looking at the sum as j goes from 0 to 3 of 0 times the probability of 0, 1 times the probability of a 1, 2 times the probability of a 2, plus 3 times the probability of a 3. And I think you'll notice that this is exactly the sum that we get, treating this as a computation of the expected value of g of x in the x probability space. So in general, what has happened here is that when we're computing the expected value of x in the probability space x, we're taking this sum, and we can group this as the values y in the sample space for the probability space of y-- that is, the values that g of x can take on.

And then inside this grouping, look at all the x's, such that g of x equals y. And take g of x, f of x, and sum it that way. We've just added an extra structure to our summation that will add up all of the g of x's that come out to a particular value, and then move on to the next value.

But this is the sum over all y's in the sample space for y of g of x. Now, it was just y f of x, x such that g of x equals y. So this will be the sum over y's in the sample space for y of y times the sum over the x's for which g of x equals y f of x. But that is just the sum over y's in the sample space for the distribution induced by y.

y times the density, with respect to the y random variable of y, because this value-- the probability of y is just the sum over all the x's that map to y. Oops. Map to y-- there we go. It's kind of fun.

This principle-- the expected value of g of x in the x probability space equals the expected value of y in the y probability space-- is called the law of the sleeping statistician. So happens for you automatically, even if you're asleep. There are some basics of expected value that are really handy to have available.

The expected value of a constant times a random variable is equal to that constant times the expected value of the random variable. And you can see that, just by looking at the definition in both cases for discrete and continuous random variables, that c just factors out of the summation or the integration. Also, the expected value of a sum of functions of a random variable is equal to the sum of the expected values. Again, just break up the integral or the summation into its g of x and h of x pieces. That's fairly straightforward.

And also, the expected value of x plus a constant equals the expected value of x plus the constant, because the expected value-- we can take the constant to be another function of x. And the expected value of the constant is just going to be the sum or integral of the constant times the density function-- so the constant times 1.

Notice that if x is a random variable, then the mean of x that we defined as the sum or integral of x times the density of x is exactly the expected value of the identity function x. Again, both sides of the equation, we're summing or integrating x times the density at x. Likewise, the variance can be defined in terms of the expectation.

The variance of x, you'll recall, is the sum, or integral, of x minus the mean of x, quantity squared, times the density. And that's just the definition of the expected value of the function of the random variable-- x minus that constant mean of x, quantity squared. And we know from derivations on the variance that this equals the sum, or integral, of x squared minus the expected value of x itself, quantity squared.

So the equality comes from previous work on the variance. And then we can translate both of those formulas into expected value terms. So this is one step on the way to understanding the distribution of a mean of samples from a given population.

# Jointly Distributed Random Variables

So continuing on with the project of working toward understanding the distribution of the mean of a sample from a population, let's take on the question of how to handle jointly distributed random variables. That is a collection of random variables on an underlying sample space.

So if X1 through Xn, for example, are random variables on a common probability space, in the sense that each of them maps the sample space of that probability space to the reals, then we say that X1 through Xn are jointly distributed random variables. A fairly simple example of this is the projection functions. So if your sample space is a subset over Rn of the Cartesian product of R with itself n times, then the projection functions, X sub i defined by X sub i applied to a vector X1 through Xn in the sample space just picks out and return the i-th entry.

These projection functions are jointly distributed random variables, straight up according to the definition. And each random variable gives rise to a probability space in the usual way. And we call that its marginal distribution.

If you have jointly distributed random variables, they give rise to a probability space the same way a single random variable gives rise to a probability space, or in a very similar way at any rate. The sample space, S prime, M prime, P prime, is a probability space. And the projection distributions are distributed as the original X1 through Xn.

So let's look at an example of the construction of jointly distributed random variables. We need an underlying sample space and two random variables on that sample space. So for the underlying sample space, let's take responses on a Likert type scale. And let's say the outcomes are disagree, somewhat disagree, somewhat agree, and agree. And they occur with probabilities 0.1, 0.2, 0.4, and 0.3 respectively, which does, in fact, add up to 1. And define the random variable X by X mapping agree and disagree both to 1.

And otherwise, X maps the outcome to 0. So X is sort of our extreme indicator. The person had a strong opinion and we code that with a 1. The respondent did not have a strong opinion and we code that with a 0. And then let's let Y just be a numeric re-encoding of the classification set-- disagree to 0, somewhat disagree to 1, somewhat agree to 2, and agree to 3.

By the way, X and Y aren't independent. The probability that X equals 1 is the 0.1 plus 0.3, so 0.4. The probability that Y equals 2 is 0.4. But the probability that X equals 1 and Y equals 2 is 0. X equals 1, agree or disagree. And Y equals 2 is somewhat agree. So, yes, the probability there is 0, rather than the product.

Another source of jointly distributed random variables is probability spaces with the sample space in Rn. So this is just a sketch. We're suppressing details about measurable sets and measurable functions and so on. But basically, a continuous probability space in Rn has a sample space that's in Rn, a reasonable set of events, and a probability function induced by a nicely behaved function F from Rn to 0 to infinity, such that for any Cartesian product of an interval in the first coordinate, cross an interval in the second coordinate, cross an interval in the n-th coordinate, the probability of that event is just the integral over the first interval with respect to dx1 over the second interval with respect to x2, dot dot dot, over the n-th interval with respect to Xn of that density function. In that case, we call this probability space a continuous probability space as well.

We can extend the concept of independent events on a probability space to independent random variables on a probability space. We say that two random variables on a probability space are independent if given any event A of the form of values that X maps to an interval and any event B of the form of values from the sample space that Y maps to an interval, then the events that X is in A and that Y is in B are independent. So this has to apply for any possible interval inducing an A, any possible interval inducing a B.

This extends to collections of random variables. We can speak of mutually independent random variables. And in this case, if X1 through Xn are random variables on a probability space, we say they're mutually independent if for any collection of events induced by intervals that X1 maps to, that X2 maps to, and so on, those events are mutually independent.

Let's look at an example of independent random variables in a discrete case. So consider the discrete probability space that consists of rolling a fair die twice independently and recording the results in order. The sample space will just be pairs of integers. The first value being the first value rolled, the second value being the second value rolled. So pairs of integers between 1 and 6. The collection of measurable sets is the power set of the sample space. And the density function we'll just say is the probability that any particular pair occurs is 1/36.

Then the projection functions, picking out the first value rolled and picking out the second value rolled are in fact jointly distributed random variables. And they're independent for any collection in 1 through 6. For any A that's a subset of 1 through 6 and any B that's a subset of 1 through 6, the probability of the event of a pair for which the first value X is an A is just the number of pairs for which that first value X is in A divided by 36. So that would be the count of values X in A times 6 divided by 36, so just the count of values of X in A divided by 6-- the 6 and the 36 canceling to give us a 6 in the denominator.

And likewise the probability of an event of the event that Y, the second roll is in a subset B is the size of the subset B divided by 6. Whereas the probability of the intersection that X is in A, and Y is in B is those outcomes that consist of an X in A paired with a Y in B, there will be size of A time size of B of those and divided by 36. So you'll note that size of A/6 times the size of B/6 multiplied together is in fact, the size of A times the size of B over 36. So we have our independence property, the probability of the intersection equals the probability of the first event times the probability of the second event multiplied together.

Let's look at an example where we have discrete random variables that aren't in fact independent. This time let's look at a probability space on the same sample space, still pairs of numbers 1 through 6 and 1 through 6 and the same set of events, but now define the density function to make pairs where the first and second value match just a bit more likely than pairs where they're different. So if we set it up with the probability that the first element in the pair equals the probability of each pair where the first and second elements are equal, we make that 2/42. And if they're unequal, we make the probability of that pair 1/42.

You can check. And once again, this is a valid probability space. The sums of these probabilities do add up to 1. So the projection functions X and Y, which pull out the first component of the pair and the second component of the pair respectively, are jointly distributed random variables according to our definition. But they're not independent.

Suppose we take A to be the event that we have a pair that maps to 6 under X. So that is the first component in the pair is 6. And B be the event that we have a pair that maps to 1 under the projection function Y. So in other words, we have a pair whose second component is 1. Then each event has the probability 2/42. So that would account for, in the case of A, 6-6 pair. And then there are another 5 possibilities for that second value Y.

So we could have X equals 6, but Y equals any of the other values 1 through 5. Each of those have probability 1/42. So together, they have probability 5/42. And the probability of the event that the first value is 6 is 2/42 plus 5/42, which is 7/42, which is just 1/6. And the same calculation goes for B.

However, the intersection requires the X value to be 6 and the Y value to be 1. That's just one pair, 6-1. It's a pair in which A is not equal to B. And so it has probability 1/42, which is not equal to the product of the other two probabilities, each of which are 1/6, giving us 1/36 instead.

In general, if you have two separate probability distributions, separate random variables, X with its own sample space, set of events and probability function, Y with its own sample space, set of events and probability function, and each of these are induced by densities f sub x and f sub y respectively, then you can define a discrete probability space on the Cartesian product of the sample space induced by the density function that the probability of or density at a pair, xi, yj, is just the product of the probability density with respect to x of xi times the probability density in the y random variable of yj.

We can also take two continuous random variables, X and Y, and create a continuous random variable in R2 from those by setting our sample space to be the Cartesian product of the two sample spaces of the random variable A and the random variable Y, and defining the probability function by extension from defining the probability of an extended rectangle, all of the ordered pairs x and y, for which x is in some set A and y is in some set B, to be the integral of the product of the density function for x evaluated at a point x times the product of the density function for the random variable y evaluated at a point y over the set x is in A with respect to x and then over the set y is in B with respect to y.

So let's do a quick check that this does indeed give us X and Y as independent random variables on this new probability space. So to calculate the probability that x is in A and y is in B, we calculate this integral. And then, we can factor out the f of y applied to y from the integral with respect to x, because it's a constant with respect to x. So let's just pull that out of the integral with respect to x. And now, we can carry out the integral with respect to x. And we see that that's just the probability of the event A in the X sample space. And then it's a constant with respect to this integral and can just be factored out, leaving us with the integral that computes the probability of the set B in the Y sample space. So the probability that x is in A and y is in B is in fact just equal to the probability of the event A times the probability of the event B as required by independence.

Again this can be extended to random variables X1 through Xn if we simply require that the events A1, A2 through An be mutually independent whenever A1 is a subset of the sample space for X1, A2 is a subset of the sample space for X2, and so on, An is a subset of the sample space for Xn.


nah, this subject lost me. 

## Summary

expectations: 