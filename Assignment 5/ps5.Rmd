---
title: "Problem Set 5 Solutions"
author: "A. Dehesa"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
options(dplyr.summarise.inform = FALSE)

```

# Introduction

As an application of the some of the properties of expected values, problems 1-7 step through a proof that the expected value of the random variable that defines sample variance is the population variance, given that the population variance is defined. 

For each of Q1- Q7, let $X_1,X_2,...X_n$ be independent, identically distributed random variables with defined mean $\mu$ and variance $\sigma^2$.

Question 8 gives examples of jointly distributed random variables that are independent and jointly distributed random variables that aren't independent.

Question 9 gives an application of the mean and variance properties of independent random variables.

This is a practice problem set intended to provide applications of material regarding jointly distributed random variables and expected values of functions of jointly distributed random variables. Questions on this material will be included on the midterm assessment.

These questions were rendered in R markdown through RStudio (<https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>, <http://rmarkdown.rstudio.com> ).

# Question 1 

(5 points)

Let $X_1,X_2,...X_n$ be independent, identically distributed random variables with mean $\mu$ and variance $\sigma^2$, and define the random variable $\bar X$ by $\bar X=\frac{1}{n}\sum_{i=1}^nX_i$. Justify the equality $$E\left[\sum_{i=1}^n\left(X_i-\bar X\right)^2\right]=E\left[\sum_{i=1}^nX_i^2\right]-2E\left[\sum_{i=1}^n\bar XX_i\right]+E\left[\sum_{i=1}^n\bar X^2\right]$$

In this case, we can treat $$E\left[\sum_{i=1}^n\left(X_i-\bar X\right)^2\right]$$

as powers of a binomial. 
In this case, $X_{i}$ is A and $\bar X$ is B. 
Therefore, (A-B)^2 = $A^2$ - $2*A*B$ + $B^2$
Now we substitute again A and B and we get: 
$$E\left[ \sum_{i=1}^n \left( X_i^2 -2*X_i*\bar X + \bar X^2 \right) \right]$$
which if we expand, would give us: 
$$E\left[\sum_{i=1}^nX_i^2\right]-2E\left[\sum_{i=1}^n\bar XX_i\right]+E\left[\sum_{i=1}^n\bar X^2\right]$$
# Question 2 

(5 points)

Let $X_1,X_2,...X_n$ be independent, identically distributed random variables with mean $\mu$ and variance $\sigma^2$.

In terms of $\mu$ and $\sigma^2$, what is the value of $E[X_i^2]$? Note that $Var[X_i]=E[X_i^2]-E[X_i]^2$, while $Var[X_i]=\sigma^2$ and $E[X_i]=\mu$. Please justify your answer.

Confirm numerically that your answer is correct for $X_i\sim gamma(shape=3,scale=2)$ which has mean equal to $6$ and variance equal to $12$. 


Since we know that 
$Var[X_i]=E[X_i^2]-E[X_i]^2$, while $Var[X_i]=\sigma^2$ and $E[X_i]=\mu$
then 
$$E \left[ X_{i}^2 \right] = \sigma^2 + \mu^2$$
```{r}
mu <- 6
variance <- 12
sigma <- sqrt(variance)
(xi <- mu^2 + sigma^2)

shape<- 3
scale <- 2

fgamma<-function(x){x^2*dgamma(x,shape=shape,scale=scale)}
integrate(fgamma,0,Inf)
```
We can see that they are both the same, or very similar. 
# Question 3 

(5 points)

Assuming that $E[X_i^2]=\sigma^2+\mu^2$ for all $i$, what is $E\left[\sum_{i=1}^nX_i^2\right]$. Recall that $E\left[\sum_{i=1}^nY_i\right]=\sum_{i=1}^nE[Y_i]$ for any random variables $Y_i,Y_2...Y_n$ with defined means.

It would simply be the sum of E[Xi^2], which is: 

$\sum_{i=1}^nE\left[X_i^2\right]$
which is: 
$\sum_{i=1}^n (\sigma^2 + \mu^2)$
which, as sigma and mu are constant, we can just multiply (sigma^2 + mu^2) by n.

# Question 4 

(5 points)

Define the random variable $\bar X$ by $\bar X=\frac{1}{n}\sum_{i=1}^nX_i$. What is the value of $E\left[\sum_{i=1}^n\bar X^2\right]$? Recall that the mean of $\bar X$ equals $\mu$ and the variance equals $\frac{\sigma^2}{n}$. The fact that $E\left[\sum_{i=1}^nY_i\right]=\sum_{i=1}^nE[Y_i]$ mentioned above may also be useful. Further, $\bar X$ is constant with respect to the index $i$ in the sum. Please justify your answer. 

If we know that $E\left[\sum_{i=1}^nY_i\right]=\sum_{i=1}^nE[Y_i]$.
Then we know that $E\left[\sum_{i=1}^n\bar X^2\right] = \sum_{i=1}^nE \left[\bar X^2\right]$

Now, from question 3, we know that $\sum_{i=1}^nE \left[\bar X^2\right] = n * E \left[\bar X^2\right]$
We also know that $E[\bar X] = \mu$ and $Var[\bar X] = \frac{\sigma^2}{n}$.
Therefore, $E[\bar X^2] = Var[\bar X] + E[\bar X]^2 = \frac{\sigma^2}{n} + \mu^2$.
Now, the sum would be just n times the result: 
$\sum_{i=1}^n E[\bar X^2] = n*(\frac{\sigma^2}{n} + \mu^2) = \sigma^2 + \mu^2n$

# Question 5 

(5 points)

Why is 
$$E\left[\sum_{i=1}^n\bar XX_i\right]=E\left[\bar X\sum_{i=1}^nX_i\right]=E\left[n\bar X^2\right]$$

We know that in $$E\left[\sum_{i=1}^n\bar XX_i\right]$$, $\bar X$ is a constant with respect to i. Therefore, it can be extracted from the sum as just a constant. 
We also know that $$\frac{1}{n}\sum_{i=1}^nX_i = \bar X$$, but in this case, we have $\sum_{i=1}^nX_i$, which is equal to $n\bar X$.
So we just multiply $\bar X * n * \bar X = n*\bar X^2$.



# Question 6 

(5 points)


Assuming that $E\left[\sum_{i=1}^nX_i^2\right]=n\left(\sigma^2+\mu^2\right)$, that $E\left[\sum_{i=1}^n\bar X^2\right]=n\left(\frac{\sigma^2}{n}+\mu^2\right)$, and that $E\left[\sum_{i=1}^n\bar XX_i\right]=E\left[n\bar X^2\right]=n\left(\frac{\sigma^2}{n}+\mu^2\right)$, please simplify $E\left[\sum_{i=1}^nX_i^2\right]-2E\left[\sum_{i=1}^n\bar XX_i\right]+E\left[\sum_{i=1}^n\bar X^2\right]$.


We have: 
$$n(\sigma^2 + \mu^2) - 2n(\frac{\sigma^2}{n} + \mu^2) + n(\frac{\sigma^2}{n} + \mu^2)$$
This can be simplified to: 
$$n(\sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2) = n(\frac{(n-1)\sigma^2}{n}) = (n-1)\sigma^2$$


# Question 7 

(5 points)

If $E\left[\sum_{i=1}^nX_i^2\right]-2E\left[\sum_{i=1}^n\bar XX_i\right]+E\left[\sum_{i=1}^n\bar X^2\right]=(n-1)\sigma^2$, what is the value of $E\left[\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar X\right)^2\right]$?


The expected value of the sample variance $(\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X )^2 )$ is equal to the population variance, σ^2. Demonstration:

$E[\frac{1}{n-1} \sum_{i=1}^n(X_i- \bar X )^2 ] = \frac{1}{n-1} E[\sum_{i=1}^n (X_i- \bar X )^2 ] = \frac{1}{n-1} [\sum_{i=1}^n E[(X_i- \bar X )^2 ]]$

From Q1
$= \frac{1}{n-1} [\sum_{i=1}^n E[X_i]^2 - 2E[X_i\bar X] + E[\bar X ]^2)]$


$= \frac{1}{n-1} *(n-1)\sigma^2$
From Q6-7

$= σ^2$.


# Question 8

## Q8, part 1

(5 points)

Consider the probability space defined by $(S,M,P)$ where $S=\{a,b,c,d,e,f\}$, the set of events $M$ is the power set of $S$, and $P$ is defined by the density $f(s)=\frac{1}{6}$ for all $s\in S$. Let $X$ be the random variable on this probability space defined by $X(a)=X(b)=X(c)=1$ and $X(d)=X(e)=X(f)=0$. Define $Y$ by $Y(a)=Y(d)=2$, $Y(b)=Y(c)=Y(e)=Y(f)=3$. Are these random variables independent?



For two random variables to be independent, P(X,Y) = P(X)P(Y)
Now, we know that:
P(X=1) = P(X(A)) + P(X(B)) + P(X(C)) = f(A) + f(B) + f(C) = 1/6 * 3 = 1/2
So P(X=0) = 1- P(X=1) = 1/2

For Y: 
P(Y=2) = Y(A) + Y(D) = 1/6 * 2 = 1/3
So P(Y=3) = 1-1/3 = 2/3

Now: 
P(X=1, Y=2) = P({A}) = 1/6
P(X=1, Y=3) = P({B,C}) = 1/3
P(X=2, Y=2) = P({D}) = 1/6
P(X=2, Y=3) = P({E,F}) = 1/3

Now, 
P(X=1, Y=2) = P(X=1)*P(Y=2)?
1/6 = 1/2*1/3? TRUE


P(X=1, Y=3) = P(X=1)*P(Y=3) = 1/3?
1/3 = 1/2*2/3? TRUE


P(X=2, Y=2) = P(X=2)*P(Y=2)?
1/6 = 1/2*1/3? TRUE


P(X=2, Y=3) = P(X=2)*P(Y=3)?
1/3 = 1/2*2/3? TRUE


Therefore, yes, they are independent. 

## Q8, part 2

(5 points)

Consider the probability space defined by $(S,M,P)$ where $S=\{a,b,c,d,e,f\}$, the set of events $M$ is the power set of $S$, and $P$ is defined by the density $f(s)=\frac{1}{6}$ for all $s\in S$. Let $X$ be the random variable on this probability space defined by $X(a)=X(d)=X(e)=1$ and $X(b)=X(c)=X(f)=0$. Define $Y$ by $Y(a)=Y(d)=2$, $Y(b)=Y(c)=Y(e)=Y(f)=3$. Are these random variables independent?


Short solution from previous question: 

P(X=1) = P(X{A,D,E}) = 1/2
P(Y=2) = P(Y{A,D}) = 1/3

P(X=1, Y=2) = P({A,D}) = 1/3

P(X=1, Y=2) = P(X=1)*P(Y=2)?
1/3 = 1/2*1/3? FALSE
Therefore, not independent


# Question 9

## Context

Suppose an experiment is performed to estimate the value of a physical constant, and that, subsequently, another experiment is performed to estimate the same value. The expected value of both experiments is the true, unknown value of the physical constant. Suppose the variance of the outcome of each experiment can be calculated from physical properties and known characteristics of the measurement technique. The two outcomes can be combined to yield a more precise estimate. 

This was the situation in 2021 regarding the value of the magnetic moment of muons. Combined experimental results gave strong evidence the previously accepted, theoretically derived value of the magnetic moment of the muon was incorrect, as shown below:

![The first result from the Muon g-2 experiment at Fermilab confirms the result from the experiment performed at Brookhaven National Lab two decades ago. Together, the two results show strong evidence that muons diverge from the Standard Model prediction. Image: Ryan Postel, Fermilab/Muon g-2 collaboration](Muon-g-2-results-plot-1024x768.jpg)
https://news.fnal.gov/2021/04/first-results-from-fermilabs-muon-g-2-experiment-strengthen-evidence-of-new-physics/ downloaded 6/27/2022

The same source provides numerical values for the accepted theoretical value for the anomalous magnetic moment,

0.00116591810

and the uncertainty:

43

the new experimental world-average anomalous magnetic moment: 0.00116592061

and the uncertainty:

41

The source states

"The combined results from Fermilab and Brookhaven show a difference with theory at a significance of 4.1 sigma"

https://news.fnal.gov/2020/06/physicists-publish-worldwide-consensus-of-muon-magnetic-moment-calculation/ downloaded 6/27/2022 gives the Brookhaven details, with uncertainty in parentheses.

"The most precise experimental result available so far is ...
 116 592 089(63) x 10-11"
 
https://cerncourier.com/a/fermilab-strengthens-muon-g-2-anomaly/ downloaded 6/27/2022 provides the Fermilab value, with uncertainty in parentheses.
 
"Fermilab...muon’s anomalous magnetic moment
 116 592 040(54)×10-11"

Generally, meta-analyses can give rise to the question of how best to combine multiple independent estimates of the same value.

The problem below investigates an approach suited to the muon situation.

## Q9 

(5 points)

Let $X$ and $Y$ be independent random variables with the same unknown mean  $\mu$ and known variances $\sigma^2_X$ and $\sigma^2_Y$. Consider $a\in [0,1]$. Let $W$ be the random variable $aX+(1-a)Y$. Note that the mean of $W$ equals $\mu$ while the variance equals $a^2\sigma^2_X+(1-a)^2\sigma^2_Y$. What value of $a$ minimizes the variance of $W$? 



We have the variance of W: 
$$Var[W] = a^2\sigma_x^2 + (1-a)^2\sigma_y^2$$
We can derivate with respect to a to get the first and second tests: 

First derivative: 

$$f´(a) = 2a\sigma_x^2 - 2\sigma_y^2(1-a)$$
Second derivative: 
$$f´´(a) = 2*(\sigma_x^2 + \sigma_y^2)$$

Now, if we equal the first derivative to 0, we get that: 
$$a = \frac{\sigma_y^2} {\sigma_x^2 + \sigma_y^2}$$

As the second derivative will always be positive, the previous result is the minimum.
Therefore, for a to be the minimum, it must be sigma_y^2 over the sum of both sigmas. 